Blöcke vs. ganze Dateien auf SSD cachen:
Es werden immer ganze Dateien gebraucht -> warum kann es Sinn machen nur einzelne Blöcke zu cachen?
Datenbankanwendung: Indexsuche machen Blöcke schon Sinn

//Häufig verwendeten Dateien entspricht häufig verwendeten Blöcken, Vermutung: kein Unterschied

Bei kleinem Cache vielleicht besser Blöcke zu cachen, wir haben großen Cache



Was möchte man überhaupt cachen?
Nur Dateiweise? Bei vielen kleinen Dateien wirkungslos
Einen ganzen Ordner (Prefetching ...)? -> gefährlich, Ordner vielleicht riesen groß

Häufig verwendete Dateien lernen?
-Zyklische Liste mit referenced Bit
Idee:
Immer die 5% am häufigsten verwendeten Dateien/Blöcke cachen (setzt lange Lernphase voraus) + Rest von großen Dateien, die nicht mit Fullspeed konsumiert werden, z.B Streaming (Prefetching, nur für Blöcke interresant)

Beim Schreiben:
Selbe zyklische Liste wie oben

Ist die Datei immer im Cache? Oder gibts Situationen in denen direkt auf Platte geschrieben wird? -> wohl nicht, zu, umfangreich
Schreibzugriffe solange verzögern bis gelesen wird

immer 4GB Platz frei, damit neue Dateien immer klomplett geschrieben werden können
Was passiert wenn zu schreibende Datei größer als SSD? -> Problem gibts bei uns nicht SSD > 4GB

Spätestens beim umount müssen alle geänderten Dateien auf HDD geschrieben werden

Ändern sich die Blocknummern im laufenden Betrieb? (z.B. bei Erweiterung/Umordnung eines Eintrags), spätestens wohl bei filesystem-check

Was passiert beim Verschieben?

Was passiert beim löschen?
SR: von SSD löschen, lösch-Kommando in queue für Festplatte hinzufügen


Auf der SSD speichern:
Komplette Dateistruktur von Festplatte (auch mit Ordnern) ablegen, zunächst nur ersten Block. Gecached Dateien sind vollständig

Beim mounten des Gesamtsystems:
Nur SSD mounten, alle infos sind ja auch auf SSD, nur nicht alle Dateien. Wenn eine Datei angefragt wird, die nicht auf SSD -> Festplatte anwerfen
Vorteil: beim Browsen ist die Festplatte nicht notwendig.
Problem: bei vielen Dateien wird SSD durch erste Blöcke voll, dann können keine ganze Dateien mehr gespeichert werden, kommt auf Blockgröße an


Fehlersicherheit:
Idee: Auf Festplatte und SSD einen Zeitstempel speichern, falls dieser Unterschiedlich ist, wird SSD geleert





SR: komplette Verzeichnisstruktur auf SSD speichern halte ich für zu umfangreich. Wenn auf beiden (SSD und HDD) FAT32 ist gelten Änderungen auch für beide -> blöd
Mein Vorschlag: erster Zugriff ganz normal auf HDD, da einklinken, in interner Struktur schauen, ob Datei auf ssd ...
Ablage struktur auf SSD: selber gemacht: array mit nummer/dateinamen und zugehörigen daten

Oder doch Clusterweise, jeder Cluster hat sowieso eindeutige Addresse, diese Adressen stehen in FAT.
Beim Laden der Chain (hole Cluster x, hole cluster y), Cluster von SSD laden

Also im Prinzip zwei mal dasselbe:
1 Dateien auf SSD verwalten, Einklinkpunkt: Dateizugriff
2 Cluster auf SSD verwalten, Einklinkpunkt: Clusterzugriff

Unterschied beim Schreiben:
Bei Clustern kann nicht auf SSD geschrieben werden, weil nicht bekannt welche Cluster zu welcher Datei gehören
Um das zu machen müsste man wieder mehr Infos auf der SSD speichern  